
## Programming with time

In this post we'll look at how to speed up (differential) dataflow computations by manipulating time. If you were looking around for a super-power to pick up, this might be the one.

Dataflow computations describe several independent bits of computation, and how each of these bits of computation should react to the arrival of new data. Unlike imperative computations, dataflow computations typically do *NOT* express any thoughts about which work should happen first, second, or last. In some interesting cases, this ambivalence can lead to sketchy performance.

Unlike vanilla dataflow systems, *timely* dataflow systems allow the programmatic manipulation of the timestamps associated with records. This allows programs to introduce sequential execution into their computation, a la carte.

### An exhausting example

Let's take the example of single-source shortest paths: we have as input a directed graph whose edges have weights (let's say strictly positive weight). For a query node, we want to produce all other nodes reachable along a directed path from the query node, and for each of these other nodes we must report the *least* path length: the sum of weights along the path.

For example, here is a picture of a directed graph with some edge weights on it:

![A directed graph](https://github.com/frankmcsherry/blog/blob/master/assets/ssswp/graph.png)

Let's talk through how we might compute the shortest paths from a supplied single source.

There is a classic algorithm that starts from the source node, A, at distance zero, and then repeatedly improves the set of shortest distances by considering single steps along edges. Once we reach a point where no further improvements can be made, we have reached the correct answer!

In the picture above we would start with node A at distance zero. Then we expand out to nodes B and C, with distances 7 and 2 respectively. In the next round we expand to node D at distance 3 and we improve the distance to B (to from 7 to 6). In each round we either reach a new node or improve the distance to an existing node, until we eventually converge.

We can write this algorithm as a differential dataflow computation, with differential dataflow's `iterate` method. In particular, we will iteratively set a collection `dists` to the result of joining `dists` with `edges`, adding whatever weight is on the edge, merging in initial distances, and retaining only the minimum distance for each node.

It looks like this:

```rust
// Set up some aliases.
type Dist = u32;
type Node = u32;
type Edge = (Node, (Node, Dist));

// Returns pairs (n, s) indicating node n can be reached from a root in s steps.
fn sssp<G: Scope>(
    edges: &Collection<G, Edge>,    // Collection of edges.
    roots: &Collection<G, Node>,    // Initial sources.
) -> Collection<G, (Node, u32)>
where G::Timestamp: Lattice+Ord {

    // Initialize roots as reaching themselves at distance 0
    let nodes = roots.map(|x| (x, 0));

    // Repeatedly update minimal distances each node can be reached from each root
    nodes.iterate(|dists| {

        // Explicitly bring into scope ...
        let edges = edges.enter(&dists.scope());
        let nodes = nodes.enter(&dists.scope());

        // New `dists` set to:
        dists.join(&edges)
             .map(|(k,l,(d,w))| (d,l+w))
             .concat(&nodes)
             .reduce(|_, s, t| t.push((*s[0].0, 1)))
     })
}
```

That might be a bit of an eyeful to look at all at once, so take your time. We will return to parts of it as we go along. The most exciting part are the four lines at the end, where we describe the rule for updating `dists`.

Let's see how a computation like this runs!

We are going to go pretty simple in this post: we'll use random graphs on specified numbers of nodes and edges, and with edge weights drawn uniformly between specified minimum and maximum values. Our computation will first load up and run `sssp` on the input graph, and then perform a sequence of 1,000 random updates to the graph (each an edge addition and deletion); we are interested in the times neede to perform both of these.

For example, let's take the easiest case of unit weights, which corresponds to the popular "breadth first search" problem:

|nodes|edges|minimum|maximum|compute|update|
|----:|----:|------:|------:|------:|-----:|
|1M   |  10M|      1|      1|  7.35s| 7.61s|

Nice! Maybe... Who even knows these days. Seven seconds might be a lot of time, but we can do a sequence of 1,000 updates in under 300 milliseconds, meaning each takes under a millisecond itself. That is great! Maybe!

Now, we are here because there is something interesting that is going to happen. Let's see what happens as we vary .. the maximum weight setting. How suspicious...

|nodes|edges|minimum|maximum|compute|update|
|----:|----:|------:|------:|------:|-----:|
|1M   |  10M|      1|      1|  7.35s| 7.61s|
|1M   |  10M|      1|      2| 13.38s|13.85s|
|1M   |  10M|      1|      3| 18.04s|18.87s|
|1M   |  10M|      1|      4| 20.74s|21.61s|
|1M   |  10M|      1|      5| 27.61s|28.73s|

Things for sure got worse. Let's crank things up even further, by orders of magnitude variation.

|nodes|edges|minimum|maximum|compute| update|
|----:|----:|------:|------:|------:|------:|
|1M   |  10M|      1|      1|  7.35s|  7.61s|
|1M   |  10M|      1|     10| 41.44s| 44.06s|
|1M   |  10M|      1|    100| 82.21s| 87.58s|
|1M   |  10M|      1|   1000|100.75s|106.96s|

Our initial running time has increased by 13x, and the incremental update times have increased 20x from 300ms to six seconds (for 1,000 updates; still not tragic).

Something has gone horribly wrong with SSSP as the variation in weights increases.

To help with our performance debugging, and because I know where this is going, let's do the same experiment where we add 1,000 to all weights. These weights have the same *absolute* range as the experiment above, but the *relative* variation is much smaller.

|nodes|edges|minimum|maximum|compute| update|
|----:|----:|------:|------:|------:|------:|
|1M   |  10M|   1001|   1001|  7.36s|  7.60s|
|1M   |  10M|   1001|   1010|  9.90s| 10.20s|
|1M   |  10M|   1001|   1100| 10.12s| 10.48s|
|1M   |  10M|   1001|   2000| 12.95s| 13.36s|

These measurements are now back to the pretty sweet numbers we saw up above. The first row is basically identical to the first row above (which makes sense: there is just one weight in both cases). The other rows, with increasing ranges of weights, never really cause problems; they are at worst the times of the two weight case up above. It turns out there is a good reason for that.

### Diagnosing the problem

Let's look at our picture again, and trace through what happens as we perform multiple rounds of iterative updates to our distances.

![A directed graph](https://github.com/frankmcsherry/blog/blob/master/assets/ssswp/graph.png)

Among other things,

* In the zeroth round we have only the root, at distance zero.

* In the first round, we produce candidate distances for each of B and C.

* In the second round, we both i. produce candidate distances for F, G, and H, and ii. improve the distance to B.

* In the third round, we propose new and improved distances to F, G, and H.

* In the fourth round, we again improve the distance to B.

* In the fifth round, we again propose new and improved distances to F, G, and H.

What you might notice here, or perhaps not because the example is somewhat small, is that we are happily investigating fairly large cost paths, the early paths through B, and informing nodes downstream about their newly discovered distances, long before we have any reason to believe that these will be final distances.

The example is actually chosen so that the shortest distance from A to H is continually changing, from 13, to 12, to 11, and eventually to 9. It takes seven rounds to get there.

Consider what will happen in a graph that has edges with weights either one or one thousand. Starting from our source, we should probably follow just weight one edges for quite a while, probably one thousand-ish steps. At that point, we can afford to follow one weight one thousand edge, and should then return to stepping along weight one edges again.

It is not even remotely helpful to step repeatedly along all of those weight one thousand edges; in fact, it is actively harmful if we propagate that information and must then eventually retract all of it (when distances to nodes eventually improve).

We could plausibly have been better off simply *waiting* to traverse high-weight edges.

### Waiting

There is a slight change we can make to the body of our loop that will delay updates to later rounds of iteration. Unfortunately, we need to drop down to *timely* dataflow, where we can directly manipulate differential dataflow's `(data, time, diff)` triples. We will just pop those open, and advance the round of iteration in `time` to be the distance reported in `data`.

Crazy, right? Probably...

```rust
    dists.join(&edges)
         .map(|(k,l,(d,w))| (d,l+w))
         .concat(&nodes)
         // BEGIN NEW
         .inner
         .map_in_place(|((d,w),t,r)|
            t.inner = std::cmp::max(t.inner, *w as u64)
         )
         .as_collection()
         // END NEW
         .reduce(|_, s, t| t.push((*s[0].0, 1)))
```

In fact, this works great. The experimental numbers above that went up to 27 seconds for five weights reduce down to:

|nodes|edges|minimum|maximum|compute|update|
|----:|----:|------:|------:|------:|-----:|
|1M   |  10M|      1|      1|  7.49s| 7.73s|
|1M   |  10M|      1|      2|  8.79s| 9.09s|
|1M   |  10M|      1|      3|  9.83s|10.32s|
|1M   |  10M|      1|      4| 10.72s|11.19s|
|1M   |  10M|      1|      5| 11.48s|11.96s|

Amazing!

And where we went up to one hundred seconds to compute and six seconds to update, we now see only:

|nodes|edges|minimum|maximum|compute| update|
|----:|----:|------:|------:|------:|------:|
|1M   |  10M|      1|      1|  7.36s|  7.60s|
|1M   |  10M|      1|     10| 15.09s| 15.99s|
|1M   |  10M|      1|    100| 34.22s| 36.17s|
|1M   |  10M|      1|   1000|127.02s|132.97s|

Eh. That was *almost* amazing. What new misery has befallen us?

### Waiting faster

Perhaps you recall learning about Dijkstra's algorithm, a sweet single-source shortest path algorithm; it is a great example of when you would learn about a [priority queue](https://en.wikipedia.org/wiki/Priority_queue), a neat data structure that lets you quickly update and extract the minimal element from a set. Dijkstra's algorithm uses a priority queue to manage the set of unvisited nodes at the least distance, and to process them in that order.

We have desperate need of such a data structure here.

Our problem is that with 1,000 distinct weights, we regularly have updates at 1,000 distinct future rounds. Differential dataflow's internal datastructures are not clever enough to behave like priority queues, and in each round will re-scan all of our updates. All that re-scanning, only to process roughly one out of each one thousand.

We can mitigate this somewhat with timely dataflow's `delay` operator, which will bundle together updates at the same time and just consider each time at most once (independent of the number of updates). We need one more line to make this happen, where we trigger the `delay` operator, using for each record the differential dataflow timestamp.

```rust
    dists.join(&edges)
         .map(|(k,l,(d,w))| (d,l+w))
         .concat(&nodes)
         .inner
         .map_in_place(|((d,w),t,r)|
            t.inner = std::cmp::max(t.inner, *w as u64)
         )
         // BEGIN NEW
         .delay(|(_,t,_),_| t.clone())
         // END NEW
         .as_collection()
         .reduce(|_, s, t| t.push((*s[0].0, 1)))
```

The result is a touch slower on the single-weight case, but it starts to dramatically improve after that.

|nodes|edges|minimum|maximum|compute| update|
|----:|----:|------:|------:|------:|------:|
|1M   |  10M|      1|      1|  8.17s|  8.39s|
|1M   |  10M|      1|     10| 14.87s| 15.74s|
|1M   |  10M|      1|    100| 25.81s| 27.82s|
|1M   |  10M|      1|   1000| 36.93s| 41.14s|

This is now much less embarassing than it was before. It isn't wonderful yet (don't ask about the COST here), but it is a significant improvement over where we started.

### Single-source *widest* paths

Let's look at another example, similar to single-source shortest paths but with a twist.

Imagine we are trying to drive a large truck through an old-timey European city, whose roads have non-uniform widths. In order to reach some address in the city, we will need to look for roads all of which are sufficiently wide to accommodate our truck. We'd like short paths, sure, but even more important is to find *wide* paths.

The single-source widest path problem looks for paths from a source node to all other nodes, with the largest *minimum* weight along the path. For each other node, the output of the computation tells us just how wide a truck we can drive to that destination.

Algorithmically this is pretty similar to the shortest path problem, but with a few changes. For pedagogical reasons we are going to flip weights over, so that we track the largest value along the path, and for each node retaining the minimum value among all paths. Just think of the value as decribing the amount of dirt used in the surrounding walls, I guess.

```rust
    roots
        .map(|x| (x, 0));
        .iterate(|widths| {

            let edges = edges.enter(&inner.scope());
            let nodes = nodes.enter(&inner.scope());

            widths
                .join(&edges)
                .map(|(k,l,(d,w))| (d, std::cmp::max(l,w)))
                .concat(&nodes)
                .reduce(|_, s, t| t.push((*s[0].0, 1)))
        })
```

With so many similarities, why are we looking at this problem? It turns out that SSWP has an interesting change from SSSP: whereas paths in SSSP naturally expire because they get longer with each hop, SSWP paths can go on for quite a while without reducing their value.

Consider a random graph with edge widths one and two. With a large average degree we can probably reach most of the nodes in the graph along a path of width two, totally ignoring paths that cross an edge of width one. At the same time, the set of nodes that can be reached along a path of width one grows exponentially faster than the set of nodes that cane be reached along a path of width two.

|nodes|edges|minimum|maximum|compute| update|
|----:|----:|------:|------:|------:|------:|
|1M   |  10M|      1|      1|  7.15s|  7.36s|
|1M   |  10M|      1|      2| 12.96s| 13.41s|
|1M   |  10M|      1|      3| 18.00s| 18.91s|
|1M   |  10M|      1|      4| 21.57s| 22.58s|
|1M   |  10M|      1|      5| 26.54s| 27.93s|
|1M   |  10M|      1|     10| 38.52s| 40.22s|
|1M   |  10M|      1|    100|100.42s|107.38s|
|1M   |  10M|      1|   1000|161.51s|178.84s|

This is a fair bit worse than SSSP (though not way out of line). Can we fix it the same way?

We can't. At least, not really. The problem is that there isn't as much of a correspondence between rounds of iteration and "width" as there was for "distance". We might take several rounds of iteration with the same width, and just delaying a shorter width one or two iterations doesn't do what we want.

What do we want? We want to punt each smaller width arbitrarily far into the future, so that we only process it once we've finished with all better widths. We can do this with fancy timely data lexicographic timestamp combinators or .. or we can just cheat a bit.

The following fragment advances each update 1024 rounds into the future based on its width. This gives each distinct width 1024 rounds to get their shit together and see which nodes can be newly reached.

```rust
    inner
        .join(&edges)
        .map(|(k,l,(d,w))| (d, std::cmp::max(l,w)))
        .concat(&nodes)
        // BEGIN NEW
        .inner
        .map_in_place(|((d,w),mut t,r)|
           t.inner = ::std::cmp::max(t.inner, 1024 * *w as u64);
        )
        .delay(|(_,t,_),_| t.clone())
        .as_collection()
        // END NEW
        .reduce(|_, s, t| t.push((*s[0].0, 1)))
```

As before, if we don't have the `delay` in there things look great until we get to larger weights, at which point things go *even more wrong*. Like, 350 seconds wrong (but then only 2 seconds to incrementally update)!

<!--
|nodes|edges|minimum|maximum|compute| update|
|----:|----:|------:|------:|------:|------:|
|1M   |  10M|      1|      1|  7.23s|  7.46s|
|1M   |  10M|      1|      2|  8.69s|  8.96s|
|1M   |  10M|      1|      3|  9.91s| 10.29s|
|1M   |  10M|      1|      4| 10.99s| 11.38s|
|1M   |  10M|      1|      5| 12.87s| 13.29s|
|1M   |  10M|      1|     10| 17.18s| 17.81s|
|1M   |  10M|      1|    100| 64.00s| 65.25s|
|1M   |  10M|      1|   1000|354.77s|356.87s|
 -->

Instead, with `delay` in place the numbers look like

|nodes|edges|minimum|maximum|compute| update|
|----:|----:|------:|------:|------:|------:|
|1M   |  10M|      1|      1|  8.00s|  8.23s|
|1M   |  10M|      1|      2|  9.32s|  9.59s|
|1M   |  10M|      1|      3| 10.24s| 10.62s|
|1M   |  10M|      1|      4| 10.96s| 11.35s|
|1M   |  10M|      1|      5| 12.10s| 12.55s|
|1M   |  10M|      1|     10| 14.74s| 15.38s|
|1M   |  10M|      1|    100| 26.93s| 28.25s|
|1M   |  10M|      1|   1000| 39.41s| 41.99s|

### Pareto surfaces

Let's get a bit more exotic, shall we?

We've done shortest paths, and we've done widest paths. Most likely there is a trade-off between the shortness of a path and its width. A path might be short but narrow, like squeezing between some buildings. Another path might be longer but wider, and more appropriate for our sweet truck. Possibly, we should be computing a spectrum of paths, so that the user can determine for themselves which is best.

In our example, paths have a length and a width, and one path is clearly better than another if it has shorter values for each (recall: we are doing the short-width is best interpretation). But we could have two paths for which neither is better than the other. Or more generally, a set of paths no one of which is better than another. What to do in this case?

We can tweak our algorithm to circulate both *lengths* and *widths* at the same time, and then rather than force `reduce` to produce a single winner, it can produce the set of mutually minimal pairs `(length, width)` as output for each node. If we iterate this process to fixed point, we find a minimal set of lengths and widths for each node. This minimal set is called a [Pareto frontier](https://en.wikipedia.org/wiki/Pareto_efficiency) by mathematicians, and a [Skyline](https://en.wikipedia.org/wiki/Skyline_operator) by database scientists.

```rust
    inner
        .join(&edges)
        .map(|(_k,((l1,w1),(d,(l2,w2))))| (d, (l1 + l2, std::cmp::max(w1, w2))))
        .concat(&nodes)
        .reduce(|_, s, t| {
            // push the first element for sure.
            t.push((*s[index].0, 1));
            for index in 1 .. s.len() {
                // push any element that improves the width.
                if (t[t.len()].0).1 > (s[index].0).1 {
                    t.push((*s[index].0, 1));
                }
            }
        })
```

If we go and run this, again on our random graphs with random weigths, we get an even-more excited degradation as we increase the variation of weights.

|nodes|edges|minimum|maximum|compute| update|
|----:|----:|------:|------:|------:|------:|
|1M   |  10M|      1|      1|  7.75s|  8.02s|
|1M   |  10M|      1|      2| 26.85s| 28.55s|
|1M   |  10M|      1|      3| 71.94s| 76.36s|
|1M   |  10M|      1|      4|128.72s|214.62s|
|1M   |  10M|      1|      5|214.86s|240.58s|

I only went out this far because it was starting to get slow.

We can improve this, as we have before. It turns out that the right thing *seems* to be to advance updates by the paths lengths, as we did for single-source shortest paths. The results improve to:

|nodes|edges|minimum|maximum|compute| update|
|----:|----:|------:|------:|------:|------:|
|1M   |  10M|      1|      1|  7.75s|  8.02s|
|1M   |  10M|      1|      2| 19.60s| 20.63s|
|1M   |  10M|      1|      3| 40.30s| 43.06s|
|1M   |  10M|      1|      4| 76.17s|114.95s|
|1M   |  10M|      1|      5|113.13s|124.38s|

Now, we *could* also have advanced updates by 1024 times the width. That worked previously, right?

It doesn't work here.

|nodes|edges|minimum|maximum|compute| update|
|----:|----:|------:|------:|------:|------:|
|1M   |  10M|      1|      1|  7.75s|  8.02s|
|1M   |  10M|      1|      2| 47.70s| 51.19s|
|1M   |  10M|      1|      3|119.43s|128.32s|
|1M   |  10M|      1|      4|215.36s|549.13s|
|1M   |  10M|      1|      5|385.52s|432.71s|

I don't have a good explanation for this. Probably it makes a lot of sense and has a sane explanation; perhaps the stratification by width forces apart many shortest path computations, but provides no structure or order for the execution of each. I'm sure it is a great mess for a real scientific reason, but I propose we don't study it further here.

### Conclusions

Most dataflow models don't normally let you specify much about *when* computation happens.
By contrast, *timely* dataflow lets you program with times, delaying data and deferring the computation it will provoke. This allows us to write programs that have finer control over which computations happen first, which allows us to write more efficient programs.

Although I haven't investigated it, my guess is that most iterative graph processors do *not* support this sort of programming with time. Systems that cannot delay processing can be stuck with less efficient execution.