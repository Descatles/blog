**IN PROGRESS**

## Data structures in Rust

Let's work through differential dataflow's main data structure: the neat and clever way we keep track of all of the changes collections experience. We are doing this both to explain the roadmap for this part of the project, but also to explain a bit about Rust's notions of ownership and borrowing, and to give examples of where things work well and where they are a bit painful.

There is much to know and learn about differential dataflow, and previous posts may be helpful with that, but for today we are just going to do something pretty localized, so ideally the surrounding complexity should just wash over us.

### Starting out

Imagine we would like to maintain a mapping from keys of type `K` to a list of pairs of times of type `T` and a list of pairs `(V, i32)`, corresponding to changes of values. For the differentially inclined, these are the changes our collection of key-value pairs have experienced at different times, indexed in a way that is convenient for when we want to use them.

Rust is pretty happy to let us write down a type that would do a fine job as our first implementation of such a map:

```rust
HashMap<K, Vec<(T, Vec<(V, i32)>)>
```

A `HashMap<T1,T2>` is just a hash map from key of type `T1` to value of type `T2`. In our case the keys are `K` and the values are

```rust
Vec<(T, Vec<(V, i32)>)>
```

where a `Vec<T1>` is just a growable vector of elements of type `T1`. It's basically just a pointer, a length, and a capacity. Here we have a `Vec` of a pair, the `( , )` tuple notation, which is just an ad-hoc struct with two fields; it lets us pair up a time `T` with a `Vec<(V, i32)>`.

This certainly seems like it lines up with our goals. What do we need to know to use one of these?

#### Ownership

Ownership is an important principle in Rust. Each live object is uniquely owned by some binding on the stack or some other live object. The owner is allowed to share access to anything it owns by using a *reference*, which we will get to later, but no one else jointly owns the data. This means that when the owner (stack binding or live object) goes out of scope, Rust can automatically collect all of its resources.

For example, if we have one of these `HashMap`s, it owns all of the `Vec<(T, ...)>`s it contains as values, each of which owns the `Vec<(V, i32)>` they contain.

If you are coming from a language that explicitly manages its memory, like C or C++, you can probably imagine that the type we've defined up above will have many allocations, and require a bunch of logic to manage all of its allocations. Maybe you don't have to write that sort of logic now, but you did at one point. And it was a pain in the butt and easy to get wrong. Here this isn't an issue: if you own a `Vec` Rust ensures that it stays in a valid state under all of its supported operations, and that when you get rid of it its memory goes away (with some minor caveats). This is even true for those types `K`, `T`, and `V` that you didn't write.

At the same time, if you come from a managed language like Java or C#, you might start to get a bit sweaty thinking about how the garbage collector is going to deal with this type. Tracing garbage collectors repeatedly crawl all the pointers in your datastructures, and if we are storing a big-data sized hunk of information here---lots of keys and timestamps and values---this would be a lot of busy-work for the garbage collector. However, because ownership is unique (each object has one owner), Rust automatically inserts the memory management logic for us: when various objects go out of scope (e.g. we remove a key and drop the associated `Vec`s) the referred-to state is collected. Rust does no work for at-rest data, unlike a tracing garbage collector.

The downside of unique ownership is that sharing can be hard. If I have some data and want to just show it to you, it seems like I either have to give up my ownership or make a second copy for you, and that could be expensive. The answer to this is references, which we will get to in just a bit.

### Generalizing (Traits)

That `HashMap` up above looks pretty sweet, but it isn't the best implementation by a long stretch. We could optimize it lots, but instead lets think about writing a more general take on this collection of data.

Rust provides the ability to define interfaces and types through *traits*. A trait lets you describe what features an implementation should have, without committing to the implementation just yet. Let's look at a quick example, the `Iterator` trait, which defines a type `Item` and a function `next`:

```rust
trait Iterator {
	type Item;
	fn next(&mut self) -> Option<Self::Item>;
}
```

When writing your own iterator, you get to indicate the type of data returned, and write the logic to determine what the next result will be (possibly `None`, one of the values `Option` allows). This makes lots of sense because you probably have opinions on how to do this. The `&mut self` might not make sense yet, but it basically just names the object itself (and gives permission to mutate it).

When using someone else's iterator, you don't really need to know too much more than the type of item returned and that there is a `next` function. Obviously, Rust's compiler will have to have a more detailed chat with that someone else's code, in order to actually produce a full program, but you don't have to worry about the details when you write your code. This is handy, because it lets us write generic code that works with any iterator.

Let's write a more complicated trait; one that looks more like the storage we want to provide. This is going to look like the `HashMap` up above, except that in our case we don't really need random write access, just to occasionally bulk-load some data.

```rust
trait Trace {
	// associated types
	type Key;	// key type; `K` above.
	type Time;	// time type; `T` above.
	type Value; // value type; `V` above.

	// takes a collection of data and introduces it into the map.
	fn insert(&mut self, data: Vec<(Self::Key, Self::Time, Self::Value, i32)>);

	// returns the information about a given key.
	fn remove(&mut self, key: K) -> Vec<(T, Vec<(V, i32)>)>;
}
```

What have we done here? We've written a trait which names three types, `Key`, `Time`, and `Value`, and two methods `insert` and `remove`. From the signatures, we can see that `insert` takes an owned `Vec` as an argument, which means that it owns everything inside; this is good because we will want to move the owned copies into our map. We also see that `remove` produces an owned `Vec`, which is a bit annoying because it means we either have to give up ownership of some data (or make copies). Maybe instead of a `remove` function we would rather have a `lookup` function?

#### References

References are types that provide access to an object, but do not own the object. A reference is written as `&T` and means "a reference to an existing (and elsewhere-owned) object `T`". If you write `&mut T` it corresponds to an exclusively held reference that you can even mutate; this means that no one else is currently working with it, not even the owner.

Rust statically guarantees that all references are to valid owned data, and in the case of mutable references that you are the exclusive reference holder. This makes using references very safe. However, it also means that you can't hold on to a reference forever. The data to which the reference refers is owned by someone else, and may become invalid if the owner mutates or discards the object.

When a reference is created, an act Rust calls *borrowing*, it acquires a "lifetime": some part of your code where the reference is guaranteed to be valid, and after which Rust can see that the reference is discarded. To ensure the reference is valid, Rust prevents you from taking further mutable references (which would allow mutation), from moving the owned object, and from taking shared references if the first was mutable.

Rust tries its best to infer an appropriate lifetime for you, but often fails. Most of the time it fails because there isn't actually a valid lifetime, because your (my) program has bugs. Sometimes it fails because there are ambiguous choices. However, once it starts working, it makes a lot of sense. Let's see an example.

To add a `lookup` method, we want to mimic the `remove` method but return a reference to the data rather than an owned copy of the data. Here it is:

```rust
    fn lookup(&mut self, key: K) -> &Vec<(T, Vec<(V, i32)>)>;
```

See how the return type has a `&` in front of it now? That means that what comes back is a reference.

So what is the lifetime of this reference? For how long can we hold on to it? Can we go and add a few things to the map with `insert` and then come back to our reference?

No.

Rust is able to infer that the lifetime of the reference must be the same as the lifetime of the reference `&mut self`: the only thing we could possibly be borrowing is the object itself (it is the only parameter that is a reference). Because the lifetime of the result is the same as the lifetime of the `&mut self` reference, which Rust implicitly takes when we called the method, holding on to the returned reference also holds on to the mutable borrow as well. This prevents any other borrows of `self` until we release the reference.

This is good! That reference is just a pointer in to memory, and if we went and either a) `remove`d the key or b) `insert`d enough to cause the `HashMap` to re-allocate, that pointer would be pointing off into space. Bad news!

However, this also prevents calling `lookup` multiple times, which seems like it should be safe. Maybe we want to check that the lists are the same for two keys; should we be able to do that?

The problem lies in the signature, which probably doesn't actually require a mutable borrow of `self`. We are probably only going to look at the object, rather than mutate it, and so we could change the signature to

```rust
    fn lookup(&self, key: K) -> &Vec<(T, Vec<(V, i32)>)>;
```

This is great because now Rust only takes a shared reference when we call the method, and we can take multiple shared references without problem. This allows us to call `lookup` multiple times and do that comparison. We still aren't able to call `insert` or `remove`, because they require `&mut self`, but that is good because they could still invalidate our memory.

One final thing: it is a bit silly to take an *owned* key. Maybe the key is a `String`, and we wouldn't want to have to make a new copy just to look up records (and then discard the key afterwards). We can change this to be a reference too:

```rust
    fn lookup(&self, key: &K) -> &Vec<(T, Vec<(V, i32)>)>;
```

That looks really pretty, but it has a problem now: there are two inputs that are references, and Rust needs to figure out the lifetime of the result. We know that our result points at memory associated with `&self`, but it is totally possible that it could point at memory associated with `&K`. We clear up the problem by being explicit about the lifetimes of each reference:

```rust
    fn lookup<'a, 'b>(&'a self, key: &'b K) -> &'a Vec<(T, Vec<(V, i32)>)>;
```

While you are distracted, I'm going to make the same change to the `key: K` argument of `remove`, who doesn't have these issues.

### Generalizing further (Traits 2)

Our trait now looks like this:

```rust
trait Trace {
	// associated types
	type Key;	// key type; `K` above.
	type Time;	// time type; `T` above.
	type Value; // value type; `V` above.

	fn insert(&mut self, data: Vec<(Self::Key, Self::Time, Self::Value, i32)>);
	fn remove(&mut self, key: &K) -> Vec<(T, Vec<(V, i32)>)>;
	fn lookup(&self, key: &K) -> &Vec<(T, Vec<(V, i32)>)>;
}
```

It's a neat trait, but it isn't really all that general. It has a bunch of baked in assumptions about how the data are going to be stored. For example: should we really have to put all of our data in a `Vec` before calling `insert`, given that we might then just move it somewhere else? No! We can fix this by letting `insert` take an arbitrary iterator over tuples `(K, T, V, i32)`:

```rust
	// insert any iterator whose items are (Key, Time, Value, i32)
	fn insert<I: Iterator<Item=(Self::Key, Self::Time, Self::Value, i32)>>(&mut self, iter: I);
```

Notice that the `Item` type is a tuple of *owned* types: we are still absolutely getting ownership here, not just passing references to values. The difference is that we are letting the caller provide the code to produce the owned values.

Here is another baked assumption: are we really going to use `Vec<(T, Vec<(V, i32)>)>` for our storage? It is going to be pretty hard to return or get a reference to one if it doesn't exist. Let's introduce a few more associated types, and do something neat.

```rust
	...
	// associated iterator types (NEW! NEW!)
	type VIterator: Iterator<Item=(V, i32)>;
	type TIterator: Iterator<Item=(T, Self::VIterator);

	// return iterators over pairs of T and iterators over (V, i32).
	fn remove(&mut self, key: &K) -> Self::TIterator;
	...
```

Here we've gone and introduced two crazy new types: `VIterator` and `TIterator`. We've also imposed constraints on them: namely, `VIterator` should produce pairs `(V, i32)`, and `TIterator` should produce pairs of `T` and that `VIterator` type.

Why do this? It lets the implementor of `Trace` specify the type of iterator it will use when you call `remove`. If you happen to have a `Vec` full of the data, that is one iterator to use. If you've stored the data in some other way, you can specify a corresponding iterator. The user of the `Trace` trait doesn't need to know which iterators you will use when they write their program, only Rust needs to know when it compiles the program.

Ok, one last method: `insert`. Take a deep breath, because it is a bit of a disaster.

#### Higher Kinded Types

Remember that `insert` takes a `&self` as an argument, and that we like it that way. Consequently, it can only return shared references to `self`. However, the `TIterator` and `VIterator` types return *owned* `T`s and `V`s, not references to them. That worked for `remove` because we were getting rid of those values anyhow, and handing back the owned data seemed fine. Not so any more.

What we would like to write is

```rust
	...
	// associated iterator types (NEW! NEW!)
	type VIterator: Iterator<Item=(&V, i32)>;
	type TIterator: Iterator<Item=(&T, Self::VIterator);

	// return iterators over pairs of T and iterators over (V, i32).
	fn lookup(&self, key: &K) -> Self::TIterator;
	...
```

This would be totally great. But there is a horrible bit of a mess of a dark secret.

The string `Iterator<Item=(&V, i32)>` doesn't actually name a type. Rather, it is a family of types. `Iterator<Item=(&'a V, i32)>` is a well defined type for each `'a`, but each are different.

Ideally we could write something like

```rust
	...
	// associated iterator types (NEW! NEW!)
	type VIterator<'a>: Iterator<Item=(&'a V, i32)>;
	type TIterator<'a>: Iterator<Item=(&'a T, Self::VIterator<'a>);

	// return iterators over pairs of T and iterators over (V, i32).
	fn lookup<'a, 'b>(&'a self, key: &'b K) -> Self::TIterator<'a>;
	...
```

but, we can't. This apparently corresponds to a concept called higher-kinded types, in which one writes about *kinds* or families of types. I say "apparently" because this is something people like to get particular about, and I don't want to set them off.

There is a cunning trick you can employ, which I'll mention in the interest of completeness, but not elaborate on in case it ends up being an anti-pattern. Roughly, we can make a version of the `Trace` trait that has an `'a` lifetime parameter, I've called mine `TraceRef<'a>`, at which point you can use `'a` in the definition of the trait. For example,


```rust
trait TraceRef<'a> {
	type VIterator: Iterator<Item=(&'a V, i32)>;
	type TIterator: Iterator<Item=(&'a T, Self::VIterator);

	fn lookup(self, key: &K) -> Self::TIterator;
}
```

Now, "that looks nice" you might say, "but which `'a` do you implement it for?" All of them.

```rust
// implement TraceRef for references to our implementor
impl<'a> TraceRef<'a> for &'a MyStruct {
	// implement all the things
}
```

See, we implement `TraceRef<'a>` for references to our struct, rather than for the struct itself. Since each reference has a lifetime we get a name for the lifetime we'll have in the iterators. It's not the most attractive answer, and involves some more baggage when we start to use these traits, but it does seem to work for now.

### Moving along

Ok, Rust lessons aside, let's talk about actual implementations of this trait.

First off, `remove` only existed up above for pedagogical reasons. You don't remove things from traces. So, we'll scratch `remove`, which is nice because it means our traces are more like append-only data structures: each are logically equivalent to an append-only list, but may have an optimized physical structure to make `lookup` fast.

#### Implementation 0: `HashMap`

I mean, let's pretend that we will do that. It's pretty easy, and roughly works. Performance is a bit of a disaster, though.

**TODO**: Actually implement and measure.

#### Implementation 1: `Basic`

The arrangement branch of the differential repo has a vanilla implementation of `Trace` named `Basic`. It is roughly a pile of pairs `(T, Vec<(V,i32)>)` and for each key `K` a linked list of offsets into these (each key gets the range from its offset to the next keys offset). This has a pretty efficient `insert` implementation: all tuples for a time `T` happen to arrive at the same time, so we drop them in to a new vector, sort by `key`, push it on the list, and then tweak the heads of the linked lists for each involved key. The `lookup` implementation walks through the linked list for `key` and iterates over the contiguous slices of data.

This works, and isn't bad in lots of cases, but it has a few defects.

1. Differential dataflow is meant to work with low latencies, which means lots and lots of distinct times. Having independent allocations for each time means lots of allocations. Lots of allocations means lots of random accesses when we need to sort through them.

2. The time-centric layout doesn't co-locate data for the same key. When we call `lookup`, we'd really like to walk through as little memory as possible, and we really have no choice here other than to do at least one random access for each time at which the key has a difference.

3. We are going to want to compact traces, which means pointing at a hunk of the trace, grinding on it without mutating it, and then quickly swapping in the results of our computation. The large number of fine-grained allocations makes this a bit of a pain.

#### Implementation 2: `LSMTrie`

The implementation I'm leaning towards is a log-structured merge trie. It's like a [log-structured merge tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree) except the data are in a trie rather than a tree. It's otherwise pretty similar; I'll explain. The reasons I like it are mostly the reasons people like LSMTs, with a few additional properties related to Rust's ownership.

A trie is a way of storing ordered sequences that compresses common prefixes. In our case it is especially easy:

From a `Vec<(K, T, V, i32)>`, sort it by `K` then by `T` then by `V`. All keys are now in a contiguous block, within each block all times are contiguous, and finally if there are repetitions of values (there shouldn't be) they will be contiguous. So, we write down each key in sequence, and then the number of distinct times associated with it. We write down each of these distinct times, and the number of distinct values associated with each of them. Finally, we write down all of the `(V, i32)` pairs.

A log-structured merge trie is a set of tries, at most one for each power-of-two size, where updates are absorbed into new small tries, and any two tries whose sizes lie in the same power-of-two interval are merged.
